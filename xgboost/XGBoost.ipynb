{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost :eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes taken from several online sources. \n",
    "\n",
    "Source code from http://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/\n",
    "http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/ http://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/and from Matt's Lecture on Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "* implementation of Gradient Boosting algorithm\n",
    "* open source library supporting Python, R, Julia on windows, linux, mac\n",
    "* author Tianqi Chen at University of Washington\n",
    "* developed by DMLC (Distributed Machine Learning Community) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review:  What is a Gradient Boosting again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ensemble method for regression and classification: many weak learners create a strong learner\n",
    "* builds sequentially typically using shallow trees for an additive model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias/Variance tradeoff for sklearn ensemble Gradient Boosting:\n",
    "* control variance by using small learning rate, subsampling (bagging), max_features to build the trees\n",
    "* boosting reduces error mainly by reducing bias by focusing on poor predictions and trying to model them better in the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes it XGBoost eXtreme? \n",
    "* focuses on improving **speed** and **performance**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Speed**: Parallelizes each regression tree building by doing the splits/branching in parallel - use multiple CPUs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Performance**: model includes a regularization term to the Loss function to better handle various on top of hyperparameters seen in sklearn implementation.\n",
    "\n",
    "     * recall the objective function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Obj(\\Theta) = L(\\theta) + \\Omega(\\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>where are loss for linear regression often is sum of squared error:  \n",
    "$$L(\\theta) = \\sum_i (y_i-\\hat{y}_i)^2\n",
    "$$ or logistic loss: $$L(\\theta) = \\sum_i[ y_i\\ln (1+e^{-\\hat{y}_i}) + (1-y_i)\\ln (1+e^{\\hat{y}_i})$$\n",
    "\n",
    "<center> For XGBoost there is a regularization term to address model complexity: $$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**gamma** prunes leaves if they don't have sufficient gain, is **T** is number of leaves and **lambda** is the L2 regularization term(ridge regression) and **w** is the vector of scores on each leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other great features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* has useful method called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can run as distributed system on multiple computers, Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* more flexibility: custom optimization objectives and evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can handle missing data - sparse aware (DMatrix API to create a sparse matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can build from existing trained models (warm_start). *sklearn GBM also has this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. install on mac\n",
    "sudo pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. setup AWS with 32 cores fedora\n",
    "http://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ssh into AWS\n",
    "\n",
    "### check number of processors:\n",
    "**cat /proc/cpuinfo | grep processor | wc -l**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. install all packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sudo dnf install gcc gcc-c++ make git unzip python python2-numpy python2-scipy python2-scikit-learn python2-pandas python2-matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. install XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**git clone --recursive https://github.com/dmlc/xgboost**\n",
    "\n",
    "**cd xgboost**\n",
    "\n",
    "**make -j32**\n",
    "\n",
    "**cd python-package**\n",
    "\n",
    "**sudo python setup.py install**\n",
    "\n",
    "-OR-\n",
    "\n",
    "** sudo pip install xgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. confirm installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**python -c \"import xgboost;print(xgboost.__version__)\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train your xgboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exit to copy script and training data from local machine to AWS instance - (or could use git clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scp -r -i xgboost-keypair.pem ../work fedora@52.53.173.84:/home/fedora/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Log back into AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ssh -i xgboost-keypair.pem fedora@52.53.185.166**\n",
    "\n",
    "**cd work**\n",
    "\n",
    "**nohup python script.py &**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**disown** # if you want to leave it running and logout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**top** #see the cpus are being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  Then Exit and Terminate your instance in AWS console if you are just using one time.  Remember you get charged for usage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb is the native booster object and XGBClassifer is the sklearn wrapper. depending on how you train/fit/cv there are different methods and paramters for each.  Using both can be usefuleof the examples.\n",
    "\n",
    "* feature importance:\n",
    "model.feature_importances_  (didn't work for me)\n",
    "<code>\n",
    "def get_xgb_imp(xgb, feat_names):\n",
    "    from numpy import array\n",
    "    imp_vals = xgb.booster().get_fscore()\n",
    "    imp_dict = {feat_names[i]:float(imp_vals.get('f'+str(i),0.)) for i in range(len(feat_names))}\n",
    "    total = array(imp_dict.values()).sum()\n",
    "    return {k:v/total for k,v in imp_dict.items()}\n",
    "\n",
    "print get_xgb_imp(xgbm,feat_names)\n",
    "<code>\n",
    "#feature names are the list of feature names\n",
    "\n",
    "early stopping hyperparameter: if the model does not improve its scoring function in n rounds it will stop building additional trees\n",
    "\n",
    "use a watchlist in train method to get evaluation metric for objective function printed to screen as you build your model for train and test metrics:\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 300\n",
    "#pining a model requires a parameter list and data set.\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "\n",
    "watchlist will look like (in this example train is left col and test logloss is right column):\n",
    "------------------new round------------------------\n",
    "Will train until validation_1 error hasn't decreased in 10 rounds.\n",
    "[0]\tvalidation_0-mlogloss:2.873181\tvalidation_1-mlogloss:2.873842\n",
    "[1]\tvalidation_0-mlogloss:2.755566\tvalidation_1-mlogloss:2.755575\n",
    "[2]\tvalidation_0-mlogloss:2.640008\tvalidation_1-mlogloss:2.640016\n",
    "[3]\tvalidation_0-mlogloss:2.528404\tvalidation_1-mlogloss:2.528415\n",
    "\n",
    "decreases as model fits data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper on XGBoost:https://arxiv.org/pdf/1603.02754v3.pdf\n",
    "\n",
    "Windows installation:https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en\n",
    "\n",
    "Mac installation:https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en\n",
    "\n",
    "Hyperparameter tuning:\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "Hadoop and XGBoost:http://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html\n",
    "\n",
    "SPARK and XGBoost: http://dmlc.ml/2016/10/26/a-full-integration-of-xgboost-and-spark.html\n",
    "\n",
    "different examples of xgboost: https://github.com/dmlc/xgboost/tree/master/demo"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
